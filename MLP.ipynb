{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_palette('husl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'(slice(None, None, None), -1)' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a116529277f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mk_fols\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mk_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a116529277f5>\u001b[0m in \u001b[0;36mk_folds\u001b[1;34m(data, k)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mclase_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mclase_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2995\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 )\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), -1)' is an invalid key"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def read_data(name):\n",
    "    data = pd.read_csv(name)\n",
    "    return data\n",
    "\n",
    "data = read_data('cardiaca.csv')\n",
    "    \n",
    "def normalize_data(arraynp):\n",
    "    mean = np.mean(arraynp,0)\n",
    "    standard = np.std(arraynp,0)\n",
    "    for i in range(0,arraynp.shape[0]):\n",
    "        arraynp[i] = np.true_divide((arraynp[i] - mean), standard)\n",
    "    return arraynp\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(x, parameters, y):\n",
    "    probs, caches = forward(x, parameters)\n",
    "    labels = (probs >= 0.5) * 1 #redondeamos prediccion\n",
    "    accuracy = np.mean(labels == y) * 100\n",
    "\n",
    "    return round(accuracy,2)\n",
    "       \n",
    "def Split(data):\n",
    "    n = data.shape[1]\n",
    "    X = data[:, :n-1]\n",
    "    y = data[:, n-1:]\n",
    "    return X, y\n",
    "    \n",
    "\n",
    "\n",
    "def k_folds(data, k):\n",
    "    y =  data[:,-1] \n",
    "    clase_0 = np.count_nonzero( y == 0)\n",
    "    clase_1 = np.count_nonzero( y == 1)\n",
    "\n",
    "\n",
    "    div0 = int(clase_0/3)\n",
    "    div1 = int(clase_1/3)\n",
    "    print('clase 0: ', div0)\n",
    "    print('clase 1: ', div1)\n",
    "\n",
    "    class_1 = data[0:clase_1, :]\n",
    "    class_0 = data[clase_1:clase_1+clase_0, :]\n",
    "\n",
    "    np.random.shuffle(class_0)\n",
    "    np.random.shuffle(class_1)\n",
    "    \n",
    "    size_fold = int(data.shape[0] / k)\n",
    "    remainder_size_fold = int(data.shape[0] % k)\n",
    "    data = data[:data.shape[0] - remainder_size_fold, :]\n",
    "\n",
    "    k_fols = []\n",
    "    idx_row0 = 0\n",
    "    idx_row1 = 0\n",
    "    for i in range(k):\n",
    "        X0 = class_0[idx_row0:idx_row0+div0, :]\n",
    "        X1 = class_1[idx_row1:idx_row1+div1, :]\n",
    "        X = np.concatenate((X0, X1))\n",
    "        k_fols.append(X)\n",
    "        idx_row0 += div0\n",
    "        idx_row1 += div1\n",
    "    return k_fols\n",
    "\n",
    "\n",
    "k_fols =  k_folds(data,3)\n",
    "\n",
    "\n",
    "#Funcion que convierte datos categoricos en numericos\n",
    "def one_hot_encode( data ):\n",
    "    values = array(data)\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)    \n",
    "    return onehot_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializacion de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicialziacion del diccionario de parametros(pesos y bias)\n",
    "def initialize_parameters(layers_dims):\n",
    "    np.random.seed(1)               \n",
    "    parameters = {} #diccionario\n",
    "    L = len(layers_dims) #cantidad de capas  \n",
    "    \n",
    "    #ignoramos la capa de entrada (indice 0)\n",
    "    for l in range(1, L):\n",
    "        #Inicializamos la matriz de pesos aleatoriamente segun distribucion normal uniforme\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "        #Inicializamos la matriz de bias en ceros\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "        #verificamos dimensiones\n",
    "        assert parameters[\"W\" + str(l)].shape == (layers_dims[l], layers_dims[l - 1])\n",
    "        assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n",
    "        \n",
    "    #retorna matriz de pesos y vector de bias para cada capa\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIONES DE ACTIVACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion sigmoidea\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIONES AUXILIARES E IMPLEMENTACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion auxiliar para forward propagation: Calcula Z = W.A + b\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b  # Z = W.A + b _____(E1)\n",
    "    #almacenamos en cache variables que usaremos en back propagation\n",
    "    cache = (A_prev, W, b) #tupla que almacena A_prev, W  y b\n",
    "    return Z, cache\n",
    "\n",
    "#Funcion auxiliar para forward propagation: Aplicar funcion de activacion\n",
    "def linear_activation_forward(A_prev, W, b, activation_fun):\n",
    "    assert activation_fun == \"sigmoid\" \n",
    "\n",
    "    if activation_fun == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z) #_____(E2)\n",
    "\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1]) #comprobamos las dimensiones de A(W*A_prev)\n",
    "\n",
    "    #almacenamos en cache variables que usaremos en back propagation:\n",
    "    #linear_cache: A_prev, W y b\n",
    "    #activation_cache: Z antes de aplicar la funcion de activacion\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "#Forward propagation: Calcula la capa de salida, al iterar sobre las otras capas\n",
    "def forward(X, parameters):\n",
    "    hidden_layers_activation_fun=\"sigmoid\"\n",
    "    A = X                           \n",
    "    caches = [] #caches por cada capa                    \n",
    "    L = len(parameters) // 2  #por cada capa hay W y b (X2)    \n",
    "\n",
    "    #ignoramos la capa de entrada (indice 0)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation_fun=hidden_layers_activation_fun)\n",
    "        caches.append(cache) #agregamos la nueva informacion de cache\n",
    "\n",
    "    #La ultima capa aplica la funcion sigmoidea siempre, sin importar que funcion hemos especificado\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation_fun=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos como funcion de costo cross entrophy\n",
    "#Recibe Al: array de probabilidades y y:vector de respuestas\n",
    "def compute_cost(AL, y):\n",
    "    m = y.shape[1]              \n",
    "    cost = - (1 / m) * np.sum(np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL))) #_____(E3)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reciben dA:vector post activacion y Z:entrada para esa activacion\n",
    "def sigmoid_gradient(dA, Z):\n",
    "    A, Z = sigmoid(Z) #Z regresa igual\n",
    "    dZ = dA * A * (1 - A) #dA * derivada(Z) _____(E9)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIONES AUXILIARES E IMPLEMENTACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion auxiliar para back propagation: Calcula la gradiente con respecto al peso, bias y A_previo(post activacion)\n",
    "#Recibe dZ:gradiente de costo con respecto a Z\n",
    "        #cache: valores de (A_prev, W, b) provenientes del forward_popagation en la capa actual.\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T) #_____(E6)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True) #_____(E7)\n",
    "    dA_prev = np.dot(W.T, dZ) #_____(E8)\n",
    "\n",
    "    #comprobamos las dimensiones de los resultados\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "#funcion auxiliar para back propagation: Selecciona la funcion de gradiente a aplicar (depende de la de activacion)\n",
    "def linear_activation_backward(dA, cache, activation_fun):\n",
    "    #linear_cache: A_prev, W y b\n",
    "    #activation_cache: Z antes de aplicar la funcion de activacion\n",
    "    linear_cache, activation_cache = cache #las separamos\n",
    "\n",
    "    if activation_fun == \"sigmoid\":\n",
    "        dZ = sigmoid_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "#Back propagation: Calcula el gradiente de la capa de salida desde la capa de salida en orden inverso\n",
    "def backward(AL, y, caches):\n",
    "    hidden_layers_activation_fun=\"sigmoid\"\n",
    "    y = y.reshape(AL.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL)) #AL -Y / AL(1-AL) _____(E4)\n",
    "    \n",
    "    #Comenzamos en la capa de salida (siempre con sigmoidea)... _____(E5)\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\")\n",
    "\n",
    "    #... y continuamos en orden inverso\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l - 1]\n",
    "        grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = linear_activation_backward(grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fun)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actualiza las matrices de pesos y los vectores de bias\n",
    "#Recibe parameters: pesos y bias de todas las capas y grads:todas las gradientes calculadas\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 #por cada capa hay W y b (X2) \n",
    "    #Por cada capa, actualizamos pesos y bias\n",
    "    for l in range(1, L + 1):\n",
    "        # theta = theta - a.gradiente\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X, y, layers_dims, learning_rate=0.01, num_iterations=1500):\n",
    "    hidden_layers_activation_fun=\"sigmoid\"\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Inicializamos los parametros\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Inicializamos la lista de costos\n",
    "    cost_list = []\n",
    "\n",
    "    # Iteramos tantas veces como se especifica\n",
    "    for i in range(num_iterations):\n",
    "        #Aplicamos forward_propagation\n",
    "        AL, caches = forward(X, parameters)\n",
    "\n",
    "        #Calcular funcion de costo\n",
    "        cost = compute_cost(AL, y)\n",
    "\n",
    "        #Aplicar back_propagation\n",
    "        grads =backward(AL, y, caches)\n",
    "\n",
    "        #Actualizar los parametros\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        #print(f\"El costo despues de {i + 1} iteraciones es: {cost:.4f}\")\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "    #Graficamos la curva de costo\n",
    "   # plt.figure(figsize=(10, 6))\n",
    "   # plt.plot(cost_list)\n",
    "    #plt.xlabel(\"Iteraciones\")\n",
    "    #plt.ylabel(\"Costo\")\n",
    "    #plt.title(f\"Curva de costo para ratio de aprendizaje = {learning_rate}\")\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr = np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4])\n",
    "s = 0\n",
    "sum_acc = 0\n",
    "acc = np.zeros((7,6),float)\n",
    "for it in range(500,4000,500):\n",
    "    for i in range(len(apr)):\n",
    "        for k in range(3):\n",
    "            data_train = np.concatenate((k_fols[(k+1)%3], k_fols[(k+2)%3]))\n",
    "            data_test = k_fols[k]\n",
    "            train_x, train_y = Split(data_train)\n",
    "            test_x, test_y = Split(data_test)\n",
    "            # Transform input data and label vector\n",
    "            x_train = train_x.T\n",
    "            y_train = train_y.T\n",
    "            #transformamos la salida\n",
    "            #y_train = one_hot_encode(y_train)\n",
    "            test_x = test_x.T\n",
    "            test_y = test_y.T\n",
    "            #Determinamos dimensiones de las capas\n",
    "            n = x_train.shape[0]\n",
    "\n",
    "            layers_dims = [n, 2*n, 3] # 1 capas oculta de 2*n neuronas, 3 neuronas de salida\n",
    "\n",
    "            theta = MLP( x_train, y_train, layers_dims, learning_rate=0.2, num_iterations=3000)\n",
    "            sum_acc += accuracy(test_x, theta, test_y)\n",
    "        acc[s,i] = sum_acc/3\n",
    "        sum_acc = 0.0\n",
    "    s = s + 1       \n",
    "\n",
    "print(\"Conjunto de Datos: \" + dataname)\n",
    "fils = [\"iter: 500\", \"iter: 1000\", \"iter: 1500\", \"iter: 2000\", \"iter: 2500\", \"iter: 3000\", \"iter: 3500\"]\n",
    "cols = [\"α: 0.01\", \"α: 0.05\", \"α: 0.1\", \"α: 0.2\", \"α: 0.3\", \"α: 0.4\"]\n",
    "df = pd.DataFrame(acc, columns=cols, index=fils)\n",
    "\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transform input data and label vector\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "\n",
    "#Determinamos dimensiones de las capas\n",
    "n = x_train.shape[0]\n",
    "\n",
    "layers_dims = [n, 2*n, 3] # 1 capas oculta de 2*n neuronas, 3 neuronas de salida\n",
    "\n",
    "theta = MLP( x_train, y_train, layers_dims, learning_rate=0.2, num_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(x_train, theta, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
